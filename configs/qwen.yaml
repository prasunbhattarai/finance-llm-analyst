model:
  model_name: "Qwen/Qwen2.5-3B-Instruct"
  cache_dir: "models/base_model"
  quantization:
    load_in_4bit: True
    bnb_4bit_compute_dtype: "float16"
    bnb_4bit_quant_type: "nf4"

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  bias: "none"
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]

training:
  num_train_epochs: 3
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 2
  learning_rate: 3e-5
  fp16: True
  eval_steps: 500
  logging_steps: 100
  save_strategy: "epoch"
  remove_unused_columns: False
  label_names: ['labels']
  eval_strategy: 'steps'
  
data:
  train_data: "../datasets/train/processed"
  test_data: "../datasets/test/processed"
  
output:
  dir: "models/fined_tuned"
