model:
  model_name: "Qwen/Qwen2.5-3B-Instruct"
  cache_dir: "./models/base_model"
  quantization:
    load_in_4bit: True
    bnb_4bit_compute_dtype: "float16"
    bnb_4bit_quant_type: "nf4"

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  bias: "none"
  task_type: TaskType.CAUSAL_LM
  target_modules: ["q_proj", "v_proj"]

training:
  num_train_epochs: 2
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 2
  learning_rate: 2e-5
  fp16: True
  eval_steps: 200
  logging_steps: 50
  save_strategy: "epoch"
  remove_unused_columns: False
  label_names: ["labels"]



data:
  path: "./datasets/processed"

output:
  dir: "./models/fined_tuned"
